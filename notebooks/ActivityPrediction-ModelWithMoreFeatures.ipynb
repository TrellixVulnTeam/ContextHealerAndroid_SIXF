{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature engineering \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Charts\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Static params\n",
    "DATA_FOLDER = 'Data/'\n",
    "DATA_FILE = 'raw_data_fixed.txt'\n",
    "\n",
    "\n",
    "class ActitrackerLR:\n",
    "    ''' Logistic Regression models\n",
    "        one for each class\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def train_models(X_train, Y_train, model_params):\n",
    "        ''' Train models iteratively \n",
    "            for each class\n",
    "        '''\n",
    "        models = [] \n",
    "        for i in xrange(Y_train.shape[1]):\n",
    "            model = LogisticRegression(**model_params)\n",
    "            y = Y_train[:,i]\n",
    "            model.fit(X_train, y)\n",
    "            models.append(model)\n",
    "        return models\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_predictions(X_test, models, num_classes=6):\n",
    "        ''' Make predictions \n",
    "            for each class \n",
    "        '''\n",
    "        predictions = np.zeros((X_test.shape[0], num_classes))\n",
    "        for i, model in enumerate(models):\n",
    "            p = model.predict_proba(X_test)\n",
    "            predictions[:,i] = p[:,1]\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    global actitracker\n",
    "    actitracker = pd.read_csv(\n",
    "        DATA_FOLDER+DATA_FILE ,\n",
    "        sep=',' ,\n",
    "        lineterminator=';' ,\n",
    "        header=None ,\n",
    "    )\n",
    "    actitracker.columns = [\n",
    "        'user' ,\n",
    "        'activity' ,\n",
    "        'timestamp' ,\n",
    "        'x-accel' ,\n",
    "        'y-accel' ,\n",
    "        'z-accel' ,\n",
    "        'NA' ,\n",
    "    ]\n",
    "    del actitracker['NA']\n",
    "\n",
    "\n",
    "def create_sessions():\n",
    "    global actitracker\n",
    "    # re-calculate time in seconds\n",
    "    actitracker['time_seconds'] = actitracker['timestamp']*10e-9\n",
    "\n",
    "    # sort by user and time \n",
    "    actitracker = actitracker.sort_values(by=['user','time_seconds'])\n",
    "\n",
    "    # create sessions\n",
    "    session_length = 100\n",
    "    actitracker['seq'] = xrange(actitracker.shape[0])\n",
    "    actitracker['session'] = actitracker.\\\n",
    "                               groupby(['user','activity'])['seq'].\\\n",
    "                               apply(lambda x: x%session_length == 0).\\\n",
    "                               fillna(0).cumsum()\n",
    "\n",
    "\n",
    "def gather_target_vars():\n",
    "    global label_lookup\n",
    "    # get session_labels \n",
    "    ohe = OneHotEncoder(sparse=False); le = LabelEncoder()\n",
    "    labels = actitracker.groupby(['user','session'])['activity'].apply(lambda x: max(x))\n",
    "    le_labels = le.fit_transform(labels)\n",
    "    ohe_labels = ohe.fit_transform(le_labels.reshape(-1,1))\n",
    "    label_lookup = { k: v for k,v in set((i, v) for i,v in np.vstack((le_labels,labels)).T) }\n",
    "    \n",
    "    # create target variables\n",
    "    Y = pd.DataFrame(ohe_labels,index=labels.index)\n",
    "    return Y,labels\n",
    "\n",
    "\n",
    "get_label = np.vectorize(lambda x: label_lookup[x])\n",
    "\n",
    "\n",
    "def feature_engineering():\n",
    "    # group by user and session\n",
    "    accel_cols = ['x-accel','y-accel','z-accel']\n",
    "    g = actitracker.loc[:,accel_cols+['user','session']].groupby(['user','session'])\n",
    "    #print 'Feature engineering : {0}'.format(g[accel_cols[0]])\n",
    "\n",
    "    # IQR function\n",
    "    def iqr(x):\n",
    "        ''' calculate IQR from array\n",
    "        '''\n",
    "        q75, q25 = np.percentile(x, [75,25])\n",
    "        return q75-q25\n",
    "\n",
    "    # calculate model cols \n",
    "    means = g[accel_cols].apply(lambda x: np.mean(x))\n",
    "    sds = g[accel_cols].apply(lambda x: np.std(x))\n",
    "    median_1 = g[accel_cols[0]].apply(lambda x: np.median(x))\n",
    "    median_2 = g[accel_cols[1]].apply(lambda x: np.median(x))\n",
    "    median_3 = g[accel_cols[2]].apply(lambda x: np.median(x))\n",
    "    iqr_1 = g[accel_cols[0]].apply(lambda x: iqr(x))\n",
    "    iqr_2 = g[accel_cols[1]].apply(lambda x: iqr(x))\n",
    "    iqr_3 = g[accel_cols[2]].apply(lambda x: iqr(x))\n",
    "    mins = g[accel_cols].apply(lambda x: np.min(x))\n",
    "    maxs = g[accel_cols].apply(lambda x: np.max(x))\n",
    "    kurtosis_1 = g[accel_cols[0]].apply(lambda x: sp.stats.kurtosis(x))\n",
    "    kurtosis_2 = g[accel_cols[1]].apply(lambda x: sp.stats.kurtosis(x))\n",
    "    kurtosis_3 = g[accel_cols[2]].apply(lambda x: sp.stats.kurtosis(x))\n",
    "    skew_1 = g[accel_cols[0]].apply(lambda x: sp.stats.skew(x))\n",
    "    skew_2 = g[accel_cols[1]].apply(lambda x: sp.stats.skew(x))\n",
    "    skew_3 = g[accel_cols[2]].apply(lambda x: sp.stats.skew(x))\n",
    "    percentiles = []\n",
    "    for i in range(10,100,10):\n",
    "        for e in range(1,4):\n",
    "            percentiles.append(eval('g[accel_cols['+str(e-1)+']].apply(lambda x: sp.percentile(x,'+str(i)+'))'))\n",
    "\n",
    "    # concat columns\n",
    "    X = pd.concat([means,\n",
    "                    sds,\n",
    "                   median_1,\n",
    "                   median_2,\n",
    "                   median_3,\n",
    "                   iqr_1,\n",
    "                   iqr_2,\n",
    "                   iqr_3,\n",
    "                   mins,\n",
    "                   maxs,\n",
    "                   kurtosis_1,\n",
    "                   kurtosis_2,\n",
    "                   kurtosis_3,\n",
    "                   skew_1,\n",
    "                   skew_2,\n",
    "                   skew_3,\n",
    "                  ]+percentiles\n",
    "                  ,axis=1)\n",
    "\n",
    "    # Scale data\n",
    "    ss = StandardScaler()\n",
    "    X = ss.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def lr_evaluate_params(c_values):\n",
    "    accuracies = []\n",
    "    log_losses = []\n",
    "    for c in c_values:\n",
    "        params = {'C':c,'max_iter':1000,'tol':1e-8}\n",
    "        models = lrmodel.train_models(X_train, Y_train, params)\n",
    "        predictions = lrmodel.make_predictions(X_test, models, 6)\n",
    "        accuracy = accuracy_score(np.argmax(Y_test, axis=1), np.argmax(predictions,axis=1))\n",
    "        ll = log_loss(Y_test, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "        log_losses.append(ll)\n",
    "    evaluation = pd.DataFrame({'C':c_values,'accuracy':accuracies,'log_loss':log_losses})\n",
    "    print evaluation\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "def lr_param_charts(c_values, accuracies, log_losses):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(np.log(c_values), accuracies, 'g')\n",
    "    plt.title(\"Change in Accuracy with Decreasing Regularization\")\n",
    "    plt.xlabel(\"Log of Inv. Regularization Strength (C)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.log(c_values), log_losses, 'b')\n",
    "    plt.title(\"Change in Log-loss with Decreasing Regularization\")\n",
    "    plt.xlabel(\"Log of Inv. Regularization Strength (C)\")\n",
    "    plt.ylabel(\"Log-loss\")\n",
    "\n",
    "\n",
    "def print_accuracy(true_category, pred_category):\n",
    "    print 'Accuracy: {}'.format(accuracy_score(true_category, pred_category ))\n",
    "    print 'Log-loss: {}'.format(log_loss(Y_test, predictions))\n",
    "\n",
    "\n",
    "def analyze_errors(true_category, pred_category, get_label=get_label):\n",
    "    errors = pred_category != true_category\n",
    "    true_labels = get_label(true_category)\n",
    "    base = np.ones(true_labels.shape)\n",
    "    error_base = pd.DataFrame({'errors':errors,\n",
    "                      'labels':true_labels,\n",
    "                      'base':base})\n",
    "    b = error_base.groupby('labels').sum()\n",
    "    error_rates = pd.DataFrame((b['errors']/b['base']).sort_values(ascending=False), columns=['Error Rate'])\n",
    "    error_rates['Total Session Counts'] = b['base']\n",
    "    return error_rates\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    classes = label_lookup.values()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot access attribute 'values' of 'DataFrameGroupBy' objects, try using the 'apply' method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a243f46f8544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccel_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'x-accel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y-accel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'z-accel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mactitracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccel_cols\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dev/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         raise AttributeError(\"%r object has no attribute %r\" %\n",
      "\u001b[0;32m/Users/dev/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_make_wrapper\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    571\u001b[0m                    \"using the 'apply' method\".format(kind, name,\n\u001b[1;32m    572\u001b[0m                                                      type(self).__name__))\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# need to setup the selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot access attribute 'values' of 'DataFrameGroupBy' objects, try using the 'apply' method"
     ]
    }
   ],
   "source": [
    "accel_cols = ['x-accel','y-accel','z-accel']\n",
    "actitracker.loc[:,accel_cols+['user','session']].groupby(['user','session']).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user  session\n",
      "1     1           Walking\n",
      "      2           Walking\n",
      "      3           Walking\n",
      "      4           Walking\n",
      "      5           Walking\n",
      "      6           Walking\n",
      "      7           Walking\n",
      "      8           Walking\n",
      "      9           Walking\n",
      "      10          Walking\n",
      "      11          Walking\n",
      "      12          Walking\n",
      "      13          Walking\n",
      "      14          Walking\n",
      "      15          Walking\n",
      "      16          Walking\n",
      "      17          Walking\n",
      "      18          Walking\n",
      "      19          Walking\n",
      "      20          Walking\n",
      "      21          Walking\n",
      "      22          Walking\n",
      "      23          Walking\n",
      "      24          Walking\n",
      "      25          Walking\n",
      "      26          Walking\n",
      "      27          Walking\n",
      "      28          Walking\n",
      "      29          Walking\n",
      "      30          Walking\n",
      "                   ...   \n",
      "36    10954       Sitting\n",
      "      10955       Sitting\n",
      "      10956       Sitting\n",
      "      10957       Sitting\n",
      "      10958       Sitting\n",
      "      10959       Sitting\n",
      "      10960       Sitting\n",
      "      10961       Sitting\n",
      "      10962       Sitting\n",
      "      10963      Standing\n",
      "      10964      Standing\n",
      "      10965      Standing\n",
      "      10966      Standing\n",
      "      10967      Standing\n",
      "      10968      Standing\n",
      "      10969      Standing\n",
      "      10970      Standing\n",
      "      10971      Standing\n",
      "      10972      Standing\n",
      "      10973      Standing\n",
      "      10974      Standing\n",
      "      10975      Standing\n",
      "      10976      Standing\n",
      "      10977      Standing\n",
      "      10978      Standing\n",
      "      10979      Standing\n",
      "      10980      Standing\n",
      "      10981      Standing\n",
      "      10982      Standing\n",
      "      10983      Standing\n",
      "Name: activity, dtype: object\n",
      "Feature engineering : <pandas.core.groupby.SeriesGroupBy object at 0x119a0c6d0>\n"
     ]
    }
   ],
   "source": [
    "load_data()\n",
    "create_sessions()\n",
    "Y, labels = gather_target_vars()\n",
    "print labels\n",
    "X = feature_engineering()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y.as_matrix(), test_size=0.33, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4003247 , -0.3805544 , -0.17791853,  0.13834895, -0.67707807,\n",
       "        0.1862355 ,  1.45689209, -0.45677004, -0.487276  ,  0.14323398,\n",
       "       -0.67898883, -0.37901384,  0.80240762,  0.50062263,  0.08050633,\n",
       "        1.41200524,  0.04279476,  0.8866245 , -0.16974649,  0.25476395,\n",
       "        1.32464603,  0.19608871,  1.25351932,  1.9168293 ,  0.90290446,\n",
       "        0.21110376,  0.13234236,  1.02353982,  0.09213821, -0.03568492,\n",
       "        1.24188142, -0.09534324, -0.15783303,  1.3654427 , -0.27739236,\n",
       "       -0.31567319,  1.45689209, -0.45677004, -0.487276  ,  1.43321083,\n",
       "       -0.67439332, -0.65683225,  1.31815458, -0.72055132, -0.68426837,\n",
       "        1.07560947, -0.72491893, -0.59336155,  1.09926409, -0.69184339,\n",
       "       -0.47611452])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost= 1.565006971\n",
      "Epoch: 0020 cost= 1.402346969\n",
      "Epoch: 0030 cost= 1.296683788\n",
      "Epoch: 0040 cost= 1.222968221\n",
      "Epoch: 0050 cost= 1.168118477\n",
      "Epoch: 0060 cost= 1.125343323\n",
      "Epoch: 0070 cost= 1.090854168\n",
      "Epoch: 0080 cost= 1.062356591\n",
      "Epoch: 0090 cost= 1.038365960\n",
      "Epoch: 0100 cost= 1.017859697\n",
      "Epoch: 0110 cost= 1.000106812\n",
      "Epoch: 0120 cost= 0.984566748\n",
      "Epoch: 0130 cost= 0.970830500\n",
      "Epoch: 0140 cost= 0.958582640\n",
      "Epoch: 0150 cost= 0.947575808\n",
      "Epoch: 0160 cost= 0.937612474\n",
      "Epoch: 0170 cost= 0.928534746\n",
      "Epoch: 0180 cost= 0.920215905\n",
      "Epoch: 0190 cost= 0.912549317\n",
      "Epoch: 0200 cost= 0.905449390\n",
      "Epoch: 0210 cost= 0.898843169\n",
      "Epoch: 0220 cost= 0.892671406\n",
      "Epoch: 0230 cost= 0.886881351\n",
      "Epoch: 0240 cost= 0.881430387\n",
      "Epoch: 0250 cost= 0.876282036\n",
      "Epoch: 0260 cost= 0.871404231\n",
      "Epoch: 0270 cost= 0.866769373\n",
      "Epoch: 0280 cost= 0.862355173\n",
      "Epoch: 0290 cost= 0.858139038\n",
      "Epoch: 0300 cost= 0.854103982\n",
      "Epoch: 0310 cost= 0.850233912\n",
      "Epoch: 0320 cost= 0.846514046\n",
      "Epoch: 0330 cost= 0.842933714\n",
      "Epoch: 0340 cost= 0.839480460\n",
      "Epoch: 0350 cost= 0.836145282\n",
      "Epoch: 0360 cost= 0.832919717\n",
      "Epoch: 0370 cost= 0.829795361\n",
      "Epoch: 0380 cost= 0.826764941\n",
      "Epoch: 0390 cost= 0.823822498\n",
      "Epoch: 0400 cost= 0.820962965\n",
      "Epoch: 0410 cost= 0.818180263\n",
      "Epoch: 0420 cost= 0.815469742\n",
      "Epoch: 0430 cost= 0.812827528\n",
      "Epoch: 0440 cost= 0.810249448\n",
      "Epoch: 0450 cost= 0.807732284\n",
      "Epoch: 0460 cost= 0.805272400\n",
      "Epoch: 0470 cost= 0.802868009\n",
      "Epoch: 0480 cost= 0.800513804\n",
      "Epoch: 0490 cost= 0.798209369\n",
      "Epoch: 0500 cost= 0.795951962\n",
      "Epoch: 0510 cost= 0.793738723\n",
      "Epoch: 0520 cost= 0.791568160\n",
      "Epoch: 0530 cost= 0.789438665\n",
      "Epoch: 0540 cost= 0.787348330\n",
      "Epoch: 0550 cost= 0.785295427\n",
      "Epoch: 0560 cost= 0.783278644\n",
      "Epoch: 0570 cost= 0.781296372\n",
      "Epoch: 0580 cost= 0.779346704\n",
      "Epoch: 0590 cost= 0.777430117\n",
      "Epoch: 0600 cost= 0.775543571\n",
      "Epoch: 0610 cost= 0.773687601\n",
      "Epoch: 0620 cost= 0.771860301\n",
      "Epoch: 0630 cost= 0.770060658\n",
      "Epoch: 0640 cost= 0.768288136\n",
      "Epoch: 0650 cost= 0.766542256\n",
      "Epoch: 0660 cost= 0.764820755\n",
      "Epoch: 0670 cost= 0.763124466\n",
      "Epoch: 0680 cost= 0.761451662\n",
      "Epoch: 0690 cost= 0.759802401\n",
      "Epoch: 0700 cost= 0.758175194\n",
      "Epoch: 0710 cost= 0.756570041\n",
      "Epoch: 0720 cost= 0.754986882\n",
      "Epoch: 0730 cost= 0.753423989\n",
      "Epoch: 0740 cost= 0.751881361\n",
      "Epoch: 0750 cost= 0.750357449\n",
      "Epoch: 0760 cost= 0.748855174\n",
      "Epoch: 0770 cost= 0.747370005\n",
      "Epoch: 0780 cost= 0.745903015\n",
      "Epoch: 0790 cost= 0.744454443\n",
      "Epoch: 0800 cost= 0.743023455\n",
      "Epoch: 0810 cost= 0.741610050\n",
      "Epoch: 0820 cost= 0.740212679\n",
      "Epoch: 0830 cost= 0.738831997\n",
      "Epoch: 0840 cost= 0.737467349\n",
      "Epoch: 0850 cost= 0.736117959\n",
      "Epoch: 0860 cost= 0.734784722\n",
      "Epoch: 0870 cost= 0.733466625\n",
      "Epoch: 0880 cost= 0.732162654\n",
      "Epoch: 0890 cost= 0.730873466\n",
      "Epoch: 0900 cost= 0.729598522\n",
      "Epoch: 0910 cost= 0.728337646\n",
      "Epoch: 0920 cost= 0.727091014\n",
      "Epoch: 0930 cost= 0.725856602\n",
      "Epoch: 0940 cost= 0.724635720\n",
      "Epoch: 0950 cost= 0.723428071\n",
      "Epoch: 0960 cost= 0.722233236\n",
      "Epoch: 0970 cost= 0.721050262\n",
      "Epoch: 0980 cost= 0.719880760\n",
      "Epoch: 0990 cost= 0.718722224\n",
      "Epoch: 1000 cost= 0.717575848\n",
      "Optimization Finished!\n",
      "Accuracy: 0.770902\n",
      "0.770902\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    # Create the model\n",
    "    \n",
    "    # tf Graph Input\n",
    "    x = tf.placeholder(tf.float32, [None, 51]) # 3 inputs\n",
    "    y = tf.placeholder(tf.float32, [None, 6]) # 6 classes\n",
    "\n",
    "    # Set model weights\n",
    "    W = tf.Variable(tf.zeros([51, 6]))\n",
    "    b = tf.Variable(tf.zeros([6]))\n",
    "\n",
    "    # Construct model\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "    # Minimize error using cross entropy\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    \n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X_train,y: Y_train})\n",
    "        \n",
    "        # Compute average loss\n",
    "        #avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}, sess)\n",
    "\n",
    "# Store variable\n",
    "_W = W.eval(sess)\n",
    "_b = b.eval(sess)\n",
    "\n",
    "\n",
    "sess.close()\n",
    "\n",
    "#Create new graph for exporting\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    # Reconstruct graph\n",
    "    x_2 = tf.placeholder(\"float\", [None, 51], name=\"input\")\n",
    "    W_2 = tf.constant(_W, name=\"constant_W\")\n",
    "    b_2 = tf.constant(_b, name=\"constant_b\")\n",
    "    y_2 = tf.nn.softmax(tf.matmul(x_2, W_2) + b_2, name=\"output\")\n",
    "\n",
    "    sess_2 = tf.Session()\n",
    "\n",
    "    init_2 = tf.initialize_all_variables();\n",
    "    sess_2.run(init_2)\n",
    "\n",
    "    \n",
    "    graph_def = g_2.as_graph_def()\n",
    "    \n",
    "    tf.train.write_graph(graph_def, 'Models','activityModelLR.pb', as_text=False)\n",
    "\n",
    "    # Test trained model\n",
    "    y__2 = tf.placeholder(\"float\", [None, 6])\n",
    "    correct_prediction_2 = tf.equal(tf.argmax(y_2, 1), tf.argmax(y__2, 1))\n",
    "    accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, \"float\"))\n",
    "    print(accuracy_2.eval({x_2: X_test, y__2: Y_test}, sess_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 1032.783569336\n",
      "Epoch: 0010 cost= 772.242614746\n",
      "Epoch: 0015 cost= 588.952575684\n",
      "Epoch: 0020 cost= 471.314880371\n",
      "Epoch: 0025 cost= 386.303588867\n",
      "Epoch: 0030 cost= 318.476348877\n",
      "Epoch: 0035 cost= 262.130615234\n",
      "Epoch: 0040 cost= 216.910949707\n",
      "Epoch: 0045 cost= 179.391952515\n",
      "Epoch: 0050 cost= 149.126266479\n",
      "Epoch: 0055 cost= 126.199195862\n",
      "Epoch: 0060 cost= 107.779678345\n",
      "Epoch: 0065 cost= 94.188552856\n",
      "Epoch: 0070 cost= 84.890579224\n",
      "Epoch: 0075 cost= 77.752685547\n",
      "Epoch: 0080 cost= 71.332267761\n",
      "Epoch: 0085 cost= 65.772644043\n",
      "Epoch: 0090 cost= 61.178318024\n",
      "Epoch: 0095 cost= 57.208366394\n",
      "Epoch: 0100 cost= 53.544254303\n",
      "Epoch: 0105 cost= 50.242332458\n",
      "Epoch: 0110 cost= 47.363910675\n",
      "Epoch: 0115 cost= 44.791553497\n",
      "Epoch: 0120 cost= 42.446357727\n",
      "Epoch: 0125 cost= 40.342063904\n",
      "Epoch: 0130 cost= 38.489135742\n",
      "Epoch: 0135 cost= 36.827781677\n",
      "Epoch: 0140 cost= 35.344345093\n",
      "Epoch: 0145 cost= 33.971736908\n",
      "Epoch: 0150 cost= 32.691791534\n",
      "Epoch: 0155 cost= 31.494247437\n",
      "Epoch: 0160 cost= 30.392204285\n",
      "Epoch: 0165 cost= 29.351730347\n",
      "Epoch: 0170 cost= 28.375799179\n",
      "Epoch: 0175 cost= 27.458295822\n",
      "Epoch: 0180 cost= 26.596683502\n",
      "Epoch: 0185 cost= 25.783231735\n",
      "Epoch: 0190 cost= 25.004131317\n",
      "Epoch: 0195 cost= 24.254526138\n",
      "Epoch: 0200 cost= 23.537141800\n",
      "Epoch: 0205 cost= 22.869590759\n",
      "Epoch: 0210 cost= 22.250246048\n",
      "Epoch: 0215 cost= 21.663581848\n",
      "Epoch: 0220 cost= 21.111995697\n",
      "Epoch: 0225 cost= 20.585655212\n",
      "Epoch: 0230 cost= 20.077058792\n",
      "Epoch: 0235 cost= 19.582197189\n",
      "Epoch: 0240 cost= 19.100252151\n",
      "Epoch: 0245 cost= 18.636098862\n",
      "Epoch: 0250 cost= 18.199806213\n",
      "Epoch: 0255 cost= 17.779014587\n",
      "Epoch: 0260 cost= 17.374732971\n",
      "Epoch: 0265 cost= 16.981863022\n",
      "Epoch: 0270 cost= 16.601470947\n",
      "Epoch: 0275 cost= 16.235740662\n",
      "Epoch: 0280 cost= 15.880410194\n",
      "Epoch: 0285 cost= 15.540378571\n",
      "Epoch: 0290 cost= 15.212518692\n",
      "Epoch: 0295 cost= 14.894360542\n",
      "Epoch: 0300 cost= 14.586064339\n",
      "Epoch: 0305 cost= 14.286317825\n",
      "Epoch: 0310 cost= 13.992302895\n",
      "Epoch: 0315 cost= 13.710745811\n",
      "Epoch: 0320 cost= 13.448269844\n",
      "Epoch: 0325 cost= 13.176816940\n",
      "Epoch: 0330 cost= 12.925165176\n",
      "Epoch: 0335 cost= 12.679443359\n",
      "Epoch: 0340 cost= 12.448447227\n",
      "Epoch: 0345 cost= 12.232840538\n",
      "Epoch: 0350 cost= 12.018815041\n",
      "Epoch: 0355 cost= 11.816046715\n",
      "Epoch: 0360 cost= 11.620629311\n",
      "Epoch: 0365 cost= 11.417055130\n",
      "Epoch: 0370 cost= 11.237813950\n",
      "Epoch: 0375 cost= 11.051497459\n",
      "Epoch: 0380 cost= 10.885268211\n",
      "Epoch: 0385 cost= 10.702246666\n",
      "Epoch: 0390 cost= 10.534976959\n",
      "Epoch: 0395 cost= 10.363748550\n",
      "Epoch: 0400 cost= 10.208486557\n",
      "Epoch: 0405 cost= 10.065418243\n",
      "Epoch: 0410 cost= 9.888698578\n",
      "Epoch: 0415 cost= 9.753075600\n",
      "Epoch: 0420 cost= 9.626887321\n",
      "Epoch: 0425 cost= 9.456718445\n",
      "Epoch: 0430 cost= 9.314064026\n",
      "Epoch: 0435 cost= 9.167655945\n",
      "Epoch: 0440 cost= 9.030187607\n",
      "Epoch: 0445 cost= 8.892831802\n",
      "Epoch: 0450 cost= 8.761917114\n",
      "Epoch: 0455 cost= 8.632843018\n",
      "Epoch: 0460 cost= 8.507134438\n",
      "Epoch: 0465 cost= 8.384546280\n",
      "Epoch: 0470 cost= 8.265064240\n",
      "Epoch: 0475 cost= 8.148497581\n",
      "Epoch: 0480 cost= 8.034634590\n",
      "Epoch: 0485 cost= 7.922577858\n",
      "Epoch: 0490 cost= 7.812711716\n",
      "Epoch: 0495 cost= 7.704573631\n",
      "Epoch: 0500 cost= 7.598705769\n",
      "Epoch: 0505 cost= 7.494224548\n",
      "Epoch: 0510 cost= 7.391243935\n",
      "Epoch: 0515 cost= 7.289550304\n",
      "Epoch: 0520 cost= 7.189334393\n",
      "Epoch: 0525 cost= 7.089978218\n",
      "Epoch: 0530 cost= 6.991626740\n",
      "Epoch: 0535 cost= 6.897498608\n",
      "Epoch: 0540 cost= 6.801414967\n",
      "Epoch: 0545 cost= 6.738637924\n",
      "Epoch: 0550 cost= 6.636456966\n",
      "Epoch: 0555 cost= 6.534252644\n",
      "Epoch: 0560 cost= 6.461081982\n",
      "Epoch: 0565 cost= 6.382767200\n",
      "Epoch: 0570 cost= 6.333369255\n",
      "Epoch: 0575 cost= 6.218777657\n",
      "Epoch: 0580 cost= 6.119481087\n",
      "Epoch: 0585 cost= 6.042017460\n",
      "Epoch: 0590 cost= 5.956012249\n",
      "Epoch: 0595 cost= 5.879042625\n",
      "Epoch: 0600 cost= 5.806366444\n",
      "Epoch: 0605 cost= 5.754866600\n",
      "Epoch: 0610 cost= 5.705831051\n",
      "Epoch: 0615 cost= 5.623004436\n",
      "Epoch: 0620 cost= 5.559116364\n",
      "Epoch: 0625 cost= 5.493349552\n",
      "Epoch: 0630 cost= 5.412551880\n",
      "Epoch: 0635 cost= 5.350264549\n",
      "Epoch: 0640 cost= 5.307605267\n",
      "Epoch: 0645 cost= 5.218461990\n",
      "Epoch: 0650 cost= 5.178302288\n",
      "Epoch: 0655 cost= 5.095358849\n",
      "Epoch: 0660 cost= 5.050536633\n",
      "Epoch: 0665 cost= 4.963114262\n",
      "Epoch: 0670 cost= 4.916020870\n",
      "Epoch: 0675 cost= 4.845127583\n",
      "Epoch: 0680 cost= 4.833030701\n",
      "Epoch: 0685 cost= 4.752184391\n",
      "Epoch: 0690 cost= 4.722861767\n",
      "Epoch: 0695 cost= 4.614389420\n",
      "Epoch: 0700 cost= 4.563227654\n",
      "Epoch: 0705 cost= 4.506741524\n",
      "Epoch: 0710 cost= 4.434638500\n",
      "Epoch: 0715 cost= 4.386387825\n",
      "Epoch: 0720 cost= 4.331614494\n",
      "Epoch: 0725 cost= 4.303899765\n",
      "Epoch: 0730 cost= 4.238837242\n",
      "Epoch: 0735 cost= 4.179426193\n",
      "Epoch: 0740 cost= 4.121562004\n",
      "Epoch: 0745 cost= 4.094131470\n",
      "Epoch: 0750 cost= 4.040512085\n",
      "Epoch: 0755 cost= 3.971073866\n",
      "Epoch: 0760 cost= 3.917055607\n",
      "Epoch: 0765 cost= 3.901921988\n",
      "Epoch: 0770 cost= 3.814089775\n",
      "Epoch: 0775 cost= 3.804787397\n",
      "Epoch: 0780 cost= 3.738619804\n",
      "Epoch: 0785 cost= 3.682454586\n",
      "Epoch: 0790 cost= 3.642152309\n",
      "Epoch: 0795 cost= 3.589500904\n",
      "Epoch: 0800 cost= 3.580498695\n",
      "Epoch: 0805 cost= 3.539763212\n",
      "Epoch: 0810 cost= 3.450536013\n",
      "Epoch: 0815 cost= 3.408420086\n",
      "Epoch: 0820 cost= 3.386655807\n",
      "Epoch: 0825 cost= 3.318473101\n",
      "Epoch: 0830 cost= 3.272993088\n",
      "Epoch: 0835 cost= 3.233102083\n",
      "Epoch: 0840 cost= 3.211814880\n",
      "Epoch: 0845 cost= 3.172985792\n",
      "Epoch: 0850 cost= 3.172017336\n",
      "Epoch: 0855 cost= 3.147634029\n",
      "Epoch: 0860 cost= 3.063397884\n",
      "Epoch: 0865 cost= 3.028513193\n",
      "Epoch: 0870 cost= 2.999265432\n",
      "Epoch: 0875 cost= 2.977187872\n",
      "Epoch: 0880 cost= 2.917147875\n",
      "Epoch: 0885 cost= 2.859644413\n",
      "Epoch: 0890 cost= 2.834233046\n",
      "Epoch: 0895 cost= 2.829424143\n",
      "Epoch: 0900 cost= 2.779110432\n",
      "Epoch: 0905 cost= 2.736534595\n",
      "Epoch: 0910 cost= 2.686854124\n",
      "Epoch: 0915 cost= 2.669718266\n",
      "Epoch: 0920 cost= 2.638499975\n",
      "Epoch: 0925 cost= 2.602406740\n",
      "Epoch: 0930 cost= 2.566675663\n",
      "Epoch: 0935 cost= 2.538681269\n",
      "Epoch: 0940 cost= 2.505502224\n",
      "Epoch: 0945 cost= 2.484629154\n",
      "Epoch: 0950 cost= 2.439601898\n",
      "Epoch: 0955 cost= 2.482171059\n",
      "Epoch: 0960 cost= 2.421644926\n",
      "Epoch: 0965 cost= 2.399026871\n",
      "Epoch: 0970 cost= 2.311069489\n",
      "Epoch: 0975 cost= 2.323088884\n",
      "Epoch: 0980 cost= 2.289405107\n",
      "Epoch: 0985 cost= 2.255448818\n",
      "Epoch: 0990 cost= 2.204027653\n",
      "Epoch: 0995 cost= 2.163655996\n",
      "Epoch: 1000 cost= 2.162874699\n",
      "Epoch: 1005 cost= 2.119316578\n",
      "Epoch: 1010 cost= 2.113138914\n",
      "Epoch: 1015 cost= 2.082614422\n",
      "Epoch: 1020 cost= 2.053617954\n",
      "Epoch: 1025 cost= 2.033531666\n",
      "Epoch: 1030 cost= 2.024354219\n",
      "Epoch: 1035 cost= 2.001987696\n",
      "Epoch: 1040 cost= 1.974427342\n",
      "Epoch: 1045 cost= 1.918313026\n",
      "Epoch: 1050 cost= 1.872138858\n",
      "Epoch: 1055 cost= 1.849885821\n",
      "Epoch: 1060 cost= 1.851145506\n",
      "Epoch: 1065 cost= 1.814531446\n",
      "Epoch: 1070 cost= 1.795579672\n",
      "Epoch: 1075 cost= 1.769296527\n",
      "Epoch: 1080 cost= 1.742025495\n",
      "Epoch: 1085 cost= 1.707281590\n",
      "Epoch: 1090 cost= 1.675355315\n",
      "Epoch: 1095 cost= 1.652804017\n",
      "Epoch: 1100 cost= 1.642666221\n",
      "Epoch: 1105 cost= 1.634857416\n",
      "Epoch: 1110 cost= 1.671010375\n",
      "Epoch: 1115 cost= 1.598634481\n",
      "Epoch: 1120 cost= 1.564191580\n",
      "Epoch: 1125 cost= 1.566954017\n",
      "Epoch: 1130 cost= 1.529981136\n",
      "Epoch: 1135 cost= 1.531996369\n",
      "Epoch: 1140 cost= 1.549025178\n",
      "Epoch: 1145 cost= 1.510360122\n",
      "Epoch: 1150 cost= 1.499022126\n",
      "Epoch: 1155 cost= 1.411175251\n",
      "Epoch: 1160 cost= 1.428793788\n",
      "Epoch: 1165 cost= 1.393727303\n",
      "Epoch: 1170 cost= 1.429469943\n",
      "Epoch: 1175 cost= 1.364484310\n",
      "Epoch: 1180 cost= 1.337908864\n",
      "Epoch: 1185 cost= 1.299728036\n",
      "Epoch: 1190 cost= 1.318476796\n",
      "Epoch: 1195 cost= 1.263112187\n",
      "Epoch: 1200 cost= 1.244084358\n",
      "Epoch: 1205 cost= 1.225983620\n",
      "Epoch: 1210 cost= 1.216221690\n",
      "Epoch: 1215 cost= 1.226874471\n",
      "Epoch: 1220 cost= 1.214105606\n",
      "Epoch: 1225 cost= 1.201493859\n",
      "Epoch: 1230 cost= 1.180709362\n",
      "Epoch: 1235 cost= 1.172521591\n",
      "Epoch: 1240 cost= 1.164188623\n",
      "Epoch: 1245 cost= 1.151862502\n",
      "Epoch: 1250 cost= 1.183851719\n",
      "Epoch: 1255 cost= 1.122935534\n",
      "Epoch: 1260 cost= 1.105587959\n",
      "Epoch: 1265 cost= 1.092711449\n",
      "Epoch: 1270 cost= 1.090816617\n",
      "Epoch: 1275 cost= 1.054605842\n",
      "Epoch: 1280 cost= 1.086466074\n",
      "Epoch: 1285 cost= 1.037483573\n",
      "Epoch: 1290 cost= 1.022683263\n",
      "Epoch: 1295 cost= 1.025533676\n",
      "Epoch: 1300 cost= 0.970981181\n",
      "Epoch: 1305 cost= 1.020523310\n",
      "Epoch: 1310 cost= 1.009323001\n",
      "Epoch: 1315 cost= 0.971700728\n",
      "Epoch: 1320 cost= 0.941382408\n",
      "Epoch: 1325 cost= 0.922529280\n",
      "Epoch: 1330 cost= 0.926540136\n",
      "Epoch: 1335 cost= 0.909268975\n",
      "Epoch: 1340 cost= 0.936083734\n",
      "Epoch: 1345 cost= 0.882939339\n",
      "Epoch: 1350 cost= 0.867654026\n",
      "Epoch: 1355 cost= 0.907151580\n",
      "Epoch: 1360 cost= 0.870382845\n",
      "Epoch: 1365 cost= 0.868207335\n",
      "Epoch: 1370 cost= 0.867204964\n",
      "Epoch: 1375 cost= 0.818598688\n",
      "Epoch: 1380 cost= 0.814161360\n",
      "Epoch: 1385 cost= 0.819413900\n",
      "Epoch: 1390 cost= 0.805867136\n",
      "Epoch: 1395 cost= 0.784382880\n",
      "Epoch: 1400 cost= 0.818612933\n",
      "Epoch: 1405 cost= 0.775174022\n",
      "Epoch: 1410 cost= 0.804222226\n",
      "Epoch: 1415 cost= 0.758037329\n",
      "Epoch: 1420 cost= 0.744428217\n",
      "Epoch: 1425 cost= 0.749652982\n",
      "Epoch: 1430 cost= 0.697402954\n",
      "Epoch: 1435 cost= 0.719965219\n",
      "Epoch: 1440 cost= 0.745538116\n",
      "Epoch: 1445 cost= 0.735289633\n",
      "Epoch: 1450 cost= 0.708339751\n",
      "Epoch: 1455 cost= 0.702492774\n",
      "Epoch: 1460 cost= 0.663755059\n",
      "Epoch: 1465 cost= 0.693308711\n",
      "Epoch: 1470 cost= 0.711569011\n",
      "Epoch: 1475 cost= 0.676423669\n",
      "Epoch: 1480 cost= 0.642094910\n",
      "Epoch: 1485 cost= 0.624000013\n",
      "Epoch: 1490 cost= 0.607525587\n",
      "Epoch: 1495 cost= 0.609331608\n",
      "Epoch: 1500 cost= 0.598906696\n",
      "Epoch: 1505 cost= 0.590971649\n",
      "Epoch: 1510 cost= 0.614741623\n",
      "Epoch: 1515 cost= 0.554307997\n",
      "Epoch: 1520 cost= 0.578962386\n",
      "Epoch: 1525 cost= 0.608871281\n",
      "Epoch: 1530 cost= 0.648139715\n",
      "Epoch: 1535 cost= 0.542769969\n",
      "Epoch: 1540 cost= 0.570672035\n",
      "Epoch: 1545 cost= 0.534585297\n",
      "Epoch: 1550 cost= 0.587957323\n",
      "Epoch: 1555 cost= 0.521834195\n",
      "Epoch: 1560 cost= 0.591703415\n",
      "Epoch: 1565 cost= 0.535252333\n",
      "Epoch: 1570 cost= 0.509560227\n",
      "Epoch: 1575 cost= 0.494834691\n",
      "Epoch: 1580 cost= 0.559992313\n",
      "Epoch: 1585 cost= 0.488957494\n",
      "Epoch: 1590 cost= 0.510975838\n",
      "Epoch: 1595 cost= 0.470502138\n",
      "Epoch: 1600 cost= 0.512487471\n",
      "Epoch: 1605 cost= 0.478428125\n",
      "Epoch: 1610 cost= 0.490131259\n",
      "Epoch: 1615 cost= 0.481068552\n",
      "Epoch: 1620 cost= 0.479219288\n",
      "Epoch: 1625 cost= 0.431525856\n",
      "Epoch: 1630 cost= 0.435254306\n",
      "Epoch: 1635 cost= 0.442341805\n",
      "Epoch: 1640 cost= 0.397407502\n",
      "Epoch: 1645 cost= 0.424721003\n",
      "Epoch: 1650 cost= 0.423410594\n",
      "Epoch: 1655 cost= 0.460646003\n",
      "Epoch: 1660 cost= 0.512285829\n",
      "Epoch: 1665 cost= 0.402455807\n",
      "Epoch: 1670 cost= 0.419116110\n",
      "Epoch: 1675 cost= 0.392039120\n",
      "Epoch: 1680 cost= 0.416948348\n",
      "Epoch: 1685 cost= 0.432608664\n",
      "Epoch: 1690 cost= 0.359069288\n",
      "Epoch: 1695 cost= 0.381225079\n",
      "Epoch: 1700 cost= 0.377005517\n",
      "Epoch: 1705 cost= 0.375452310\n",
      "Epoch: 1710 cost= 0.370271534\n",
      "Epoch: 1715 cost= 0.350095898\n",
      "Epoch: 1720 cost= 0.352470607\n",
      "Epoch: 1725 cost= 0.402305514\n",
      "Epoch: 1730 cost= 0.369046181\n",
      "Epoch: 1735 cost= 0.336405307\n",
      "Epoch: 1740 cost= 0.350763798\n",
      "Epoch: 1745 cost= 0.356867224\n",
      "Epoch: 1750 cost= 0.421708673\n",
      "Epoch: 1755 cost= 0.345161408\n",
      "Epoch: 1760 cost= 0.338509411\n",
      "Epoch: 1765 cost= 0.326010138\n",
      "Epoch: 1770 cost= 0.316786200\n",
      "Epoch: 1775 cost= 0.302850604\n",
      "Epoch: 1780 cost= 0.315357685\n",
      "Epoch: 1785 cost= 0.305208057\n",
      "Epoch: 1790 cost= 0.308481038\n",
      "Epoch: 1795 cost= 0.322337985\n",
      "Epoch: 1800 cost= 0.282098353\n",
      "Epoch: 1805 cost= 0.295064360\n",
      "Epoch: 1810 cost= 0.329199165\n",
      "Epoch: 1815 cost= 0.365985781\n",
      "Epoch: 1820 cost= 0.295743793\n",
      "Epoch: 1825 cost= 0.293510675\n",
      "Epoch: 1830 cost= 0.271765113\n",
      "Epoch: 1835 cost= 0.289475173\n",
      "Epoch: 1840 cost= 0.312984347\n",
      "Epoch: 1845 cost= 0.273787171\n",
      "Epoch: 1850 cost= 0.254756421\n",
      "Epoch: 1855 cost= 0.302696794\n",
      "Epoch: 1860 cost= 0.313660949\n",
      "Epoch: 1865 cost= 0.290556967\n",
      "Epoch: 1870 cost= 0.298093081\n",
      "Epoch: 1875 cost= 0.248465374\n",
      "Epoch: 1880 cost= 0.220697835\n",
      "Epoch: 1885 cost= 0.249993786\n",
      "Epoch: 1890 cost= 0.234393790\n",
      "Epoch: 1895 cost= 0.236770377\n",
      "Epoch: 1900 cost= 0.273998857\n",
      "Epoch: 1905 cost= 0.248911515\n",
      "Epoch: 1910 cost= 0.222135961\n",
      "Epoch: 1915 cost= 0.229329020\n",
      "Epoch: 1920 cost= 0.218682170\n",
      "Epoch: 1925 cost= 0.243347824\n",
      "Epoch: 1930 cost= 0.252575636\n",
      "Epoch: 1935 cost= 0.214470059\n",
      "Epoch: 1940 cost= 0.193669930\n",
      "Epoch: 1945 cost= 0.234816656\n",
      "Epoch: 1950 cost= 0.234144062\n",
      "Epoch: 1955 cost= 0.252650112\n",
      "Epoch: 1960 cost= 0.297657788\n",
      "Epoch: 1965 cost= 0.218430862\n",
      "Epoch: 1970 cost= 0.227129698\n",
      "Epoch: 1975 cost= 0.173611104\n",
      "Epoch: 1980 cost= 0.185772374\n",
      "Epoch: 1985 cost= 0.257507414\n",
      "Epoch: 1990 cost= 0.195628509\n",
      "Epoch: 1995 cost= 0.242323071\n",
      "Epoch: 2000 cost= 0.171287134\n",
      "Optimization Finished!\n",
      "Accuracy: 0.874037\n",
      "0.874037\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2000\n",
    "batch_size = 500\n",
    "display_step = 5\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 200 # 1st layer number of features\n",
    "n_hidden_2 = 200 # 2nd layer number of features\n",
    "n_input = 51 # Number of inputs\n",
    "n_classes = 6 # Number of classes\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # model inputs\n",
    "    x = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    y = tf.placeholder(\"float\", shape=[None, n_classes])\n",
    "    \n",
    "    # set model weights\n",
    "    W_h1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
    "    W_h2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "    W_out = tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    \n",
    "    # set model biases\n",
    "    b1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "    b2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "    b_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "    \n",
    "    # Construct Model\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, W_h1), b1)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W_h2), b2)\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    pred = tf.matmul(layer_2, W_out) + b_out\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    sess = tf.Session()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X_train,y: Y_train})\n",
    "        \n",
    "        # Compute average loss\n",
    "        #avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}, sess)\n",
    "\n",
    "# Store Variable\n",
    "_W_h1 = W_h1.eval(sess)\n",
    "_W_h2 = W_h2.eval(sess)\n",
    "_W_out =W_out.eval(sess)\n",
    "\n",
    "_b1 = b1.eval(sess)\n",
    "_b2 = b2.eval(sess)\n",
    "_b_out = b_out.eval(sess)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# create a new graph for exporting\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    # Reconstruct Graph\n",
    "    # model inputs\n",
    "    x_2 = tf.placeholder(\"float\", shape=[None, n_input], name=\"input\")\n",
    "    \n",
    "    \n",
    "    # set model weights\n",
    "    W_2_h1 = tf.constant(_W_h1, name=\"constant_W_h1\")\n",
    "    W_2_h2 = tf.constant(_W_h2, name=\"constant_W_h2\")\n",
    "    W_2_out = tf.constant(_W_out, name=\"constant_W_out\")\n",
    "    \n",
    "    # set model biases\n",
    "    b_2_1 = tf.constant(_b1, name=\"constant_b1\")\n",
    "    b_2_2 = tf.constant(_b2, name=\"constant_b2\")\n",
    "    b_2_out = tf.constant(_b_out, name=\"constant_b_out\")\n",
    "    \n",
    "    # Construct Model\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2_1 = tf.add(tf.matmul(x_2, W_2_h1), b_2_1)\n",
    "    layer_2_1 = tf.nn.relu(layer_2_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2_2 = tf.add(tf.matmul(layer_2_1, W_2_h2), b_2_2)\n",
    "    layer_2_2 = tf.nn.relu(layer_2_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    y_2 = tf.nn.bias_add(tf.matmul(layer_2_2, W_2_out), b_2_out, name=\"output\")\n",
    "    \n",
    "    #y_2.name = \"output\"\n",
    "    \n",
    "    sess_2 = tf.Session()\n",
    "\n",
    "    init_2 = tf.initialize_all_variables();\n",
    "    sess_2.run(init_2)\n",
    "\n",
    "    \n",
    "    graph_def = g_2.as_graph_def()\n",
    "    \n",
    "    tf.train.write_graph(graph_def, 'Models','activityModelMLP2.pb', as_text=False)\n",
    "\n",
    "    # Test trained model\n",
    "    y__2 = tf.placeholder(\"float\", [None, 6])\n",
    "    correct_prediction_2 = tf.equal(tf.argmax(y_2, 1), tf.argmax(y__2, 1))\n",
    "    accuracy_2 = tf.reduce_mean(tf.cast(correct_prediction_2, \"float\"))\n",
    "    print(accuracy_2.eval({x_2: X_test, y__2: Y_test}, sess_2))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_X_test.to_csv('test_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Y_test = pd.DataFrame(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Y_test.to_csv('test_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
